# -*- coding: utf-8 -*-
"""Method2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_lzOgiSct6zJUA6qjF1PoL0qq7SB-QrE
"""

# Run this cell first (should be fast)
!pip install -q scikit-learn pandas matplotlib seaborn tqdm

import os, sys, math, pickle
from collections import Counter, defaultdict
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from tqdm import tqdm

RSEED = 42
random.seed(RSEED)
np.random.seed(RSEED)

from google.colab import drive
drive.mount('/content/drive')
dir_path = "/content/drive/MyDrive"

CORPUS_PATH = "/content/drive/MyDrive/Data/Data/corpus.txt"
if not os.path.exists(CORPUS_PATH):
    print("corpus.txt not found. In Colab: left Files pane -> Upload -> choose your corpus.txt")
else:
    print("Found corpus.txt")

# Load words
def load_words(path):
    with open(path, 'r', encoding='utf-8') as f:
        raw = [ln.strip() for ln in f if ln.strip()]
    words = [w.lower() for w in raw if w]
    # Filter empty and optionally keep only alphabetic words
    words = [w for w in words if len(w) > 0]
    return words

words = load_words(CORPUS_PATH)
print(f"Total words loaded: {len(words)}")

lengths = np.array([len(w) for w in words])
print("min, median, mean, 90pct, 99pct lengths:", lengths.min(), np.median(lengths), lengths.mean(), np.percentile(lengths,90), np.percentile(lengths,99))

# show counts for top lengths
len_counts = Counter(lengths)
top_lengths = sorted(len_counts.items())
print("length => count (sample):", top_lengths[:12])

# plot length histogram
plt.figure(figsize=(10,3))
sns.histplot(lengths, bins=range(1, int(lengths.max())+2))
plt.title("Word length distribution")
plt.xlabel("Length")
plt.show()

# overall letter frequency (only a-z)
all_letters = ''.join(words)
letter_counts = Counter([c for c in all_letters if c.isalpha()])
letters, counts = zip(*sorted(letter_counts.items()))
plt.figure(figsize=(10,3))
sns.barplot(x=list(letters), y=list(counts))
plt.title("Letter frequency (overall)")
plt.show()

# Parameters you can tweak:
min_examples_per_length = 300   # minimum words to train a per-length HMM
max_len_cap = int(np.percentile(lengths, 99))  # cap for outliers

print("max_len_cap (99pct):", max_len_cap)

# group words by exact length
words_by_len = defaultdict(list)
for w in words:
    L = len(w)
    if L > max_len_cap:
        continue  # drop extreme outliers (or put into 'long' bucket)
    words_by_len[L].append(w)

# decide which lengths will get their own HMM
lengths_to_train = [L for L,c in words_by_len.items() if len(c) >= min_examples_per_length]
lengths_to_train = sorted(lengths_to_train)
print("Exact lengths with enough examples:", lengths_to_train)

# create buckets for the rest (short/medium/long)
buckets = {'short': [], 'medium': [], 'long': []}
for L, wlist in words_by_len.items():
    if L in lengths_to_train:
        continue
    if L <= 4:
        buckets['short'].extend(wlist)
    elif L <= 8:
        buckets['medium'].extend(wlist)
    else:
        buckets['long'].extend(wlist)

for k in buckets:
    print(f"Bucket {k}: {len(buckets[k])} words")

# Build alphabet from corpus (keep only letters a-z)
alphabet = sorted(set(ch for ch in all_letters if ch.isalpha()))
alphabet = [c for c in alphabet if 'a' <= c <= 'z']  # ensure a-z
print("alphabet:", ''.join(alphabet))
letter2idx = {c:i for i,c in enumerate(alphabet)}
idx2letter = {i:c for c,i in letter2idx.items()}
V = len(letter2idx)
print("V (alphabet size) =", V)

# Numerically-stable discrete HMM with Baum-Welch (EM)
import numpy as np

def logsumexp(a, axis=None):
    a = np.asarray(a)
    a_max = np.max(a, axis=axis, keepdims=True)
    s = np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))
    return (a_max + s).squeeze(axis)

class DiscreteHMM:
    def __init__(self, n_states, n_symbols, random_state=42, eps=1e-12):
        self.n_states = n_states
        self.n_symbols = n_symbols
        self.random_state = random_state
        self.eps = eps
        rng = np.random.RandomState(random_state)
        self.startprob_ = rng.rand(n_states)
        self.startprob_ /= self.startprob_.sum()
        self.transmat_ = rng.rand(n_states, n_states)
        self.transmat_ /= self.transmat_.sum(axis=1, keepdims=True)
        self.emissionprob_ = rng.rand(n_states, n_symbols)
        self.emissionprob_ /= self.emissionprob_.sum(axis=1, keepdims=True)
        # log forms will be computed when needed

    def _to_log(self):
        self.log_startprob_ = np.log(self.startprob_ + self.eps)
        self.log_transmat_ = np.log(self.transmat_ + self.eps)
        self.log_emissionprob_ = np.log(self.emissionprob_ + self.eps)

    def _forward_log(self, obs):
        T = len(obs)
        N = self.n_states
        logalpha = np.full((T, N), -np.inf)
        logalpha[0] = self.log_startprob_ + self.log_emissionprob_[:, obs[0]]
        for t in range(1, T):
            prev = logalpha[t-1]
            temp = prev[:, None] + self.log_transmat_
            logalpha[t] = logsumexp(temp, axis=0) + self.log_emissionprob_[:, obs[t]]
        loglik = logsumexp(logalpha[-1], axis=0)
        return logalpha, loglik

    def _backward_log(self, obs):
        T = len(obs)
        N = self.n_states
        logbeta = np.full((T, N), -np.inf)
        logbeta[-1] = 0.0
        for t in range(T-2, -1, -1):
            temp = self.log_transmat_ + (self.log_emissionprob_[:, obs[t+1]] + logbeta[t+1])[None, :]
            logbeta[t] = logsumexp(temp, axis=1)
        return logbeta

    def score(self, obs):
        self._to_log()
        _, ll = self._forward_log(obs)
        return float(ll)

    def fit(self, sequences, n_iter=50, tol=1e-4, verbose=False):
        # sequences: list of 1D int arrays
        N = self.n_states
        M = self.n_symbols
        prev_ll = None
        for it in range(n_iter):
            start_counts = np.zeros(N)
            trans_counts = np.zeros((N, N))
            emit_counts = np.zeros((N, M))
            total_ll = 0.0
            self._to_log()
            for obs in sequences:
                T = len(obs)
                if T == 0:
                    continue
                logalpha, loglik = self._forward_log(obs)
                logbeta = self._backward_log(obs)
                total_ll += loglik
                loggamma = logalpha + logbeta - loglik
                gamma = np.exp(loggamma)
                start_counts += gamma[0]
                for t in range(T):
                    emit_counts[:, obs[t]] += gamma[t]
                if T > 1:
                    for t in range(T-1):
                        temp = (logalpha[t][:, None] +
                                self.log_transmat_ +
                                (self.log_emissionprob_[:, obs[t+1]] + logbeta[t+1])[None, :])
                        logxi = temp - loglik
                        xi = np.exp(logxi)
                        trans_counts += xi
            # M-step with tiny smoothing
            self.startprob_ = start_counts + 1e-8
            self.startprob_ /= self.startprob_.sum()
            row_sums = trans_counts.sum(axis=1, keepdims=True)
            row_sums[row_sums==0] = 1.0
            self.transmat_ = (trans_counts + 1e-8) / row_sums
            e_row_sums = emit_counts.sum(axis=1, keepdims=True)
            e_row_sums[e_row_sums==0] = 1.0
            self.emissionprob_ = (emit_counts + 1e-8) / e_row_sums
            if verbose:
                print(f"Iter {it+1}: total_loglik = {total_ll:.6f}")
            if prev_ll is not None and abs(total_ll - prev_ll) < tol:
                if verbose:
                    print("Converged.")
                break
            prev_ll = total_ll
        self._to_log()
        return self

def word_to_obs(word):
    return np.array([letter2idx[c] for c in word], dtype=np.int64)
def obs_to_word(obs):
    return ''.join(idx2letter[int(i)] for i in obs)

def train_hmm_with_K_sweep(word_list, K_options=[8,16,32], n_iter=80, test_size=0.15, verbose=False):
    """
    word_list: list of words (strings) for this length/bucket
    returns: best_model, best_K, dict of {K: avg_val_ll}
    """
    # prepare sequences
    word_list = [w for w in word_list if all(ch in letter2idx for ch in w)]
    train, val = train_test_split(word_list, test_size=test_size, random_state=RSEED)
    seqs_train = [word_to_obs(w) for w in train]
    results = {}
    models_for_K = {}
    for K in K_options:
        print(f"Training K={K} on {len(train)} words ...")
        model = DiscreteHMM(n_states=K, n_symbols=V, random_state=RSEED)
        model.fit(seqs_train, n_iter=n_iter, tol=1e-4, verbose=verbose)
        # eval on val
        val_lls = []
        for w in val:
            try:
                val_lls.append(model.score(word_to_obs(w)))
            except:
                val_lls.append(float('-inf'))
        avg_val_ll = float(np.mean(val_lls))
        results[K] = avg_val_ll
        models_for_K[K] = model
        print(f"K={K} avg_val_ll={avg_val_ll:.3f}")
    # pick best K (highest avg val ll)
    best_K = max(results, key=lambda k: results[k])
    best_model = models_for_K[best_K]
    return best_model, best_K, results

# Hyperparams
K_options = [8, 16, 32]
n_iter = 80

models = {}           # key -> model and meta
logprob_by_key = {}   # key -> list of (word, logprob) precomputed

# Train per-length models where enough data
for L in lengths_to_train:
    wlist = words_by_len[L]
    print(f"\n== Training for exact length {L}, words={len(wlist)} ==")
    best_model, best_K, stats = train_hmm_with_K_sweep(wlist, K_options=K_options, n_iter=n_iter, test_size=0.15, verbose=False)
    key = f"L{L}"
    models[key] = {'model': best_model, 'type': 'length', 'length': L, 'best_K': best_K, 'n_words': len(wlist), 'val_stats': stats}
    # precompute logliks
    entries = []
    for w in wlist:
        try:
            ll = float(best_model.score(word_to_obs(w)))
        except:
            ll = -1e9
        entries.append((w, ll))
    entries.sort(key=lambda x: x[1], reverse=True)
    logprob_by_key[key] = entries[:50000]  # limit if extremely long

# Train bucket models for the buckets (if non-empty)
for bname, wlist in buckets.items():
    if len(wlist) == 0:
        continue
    print(f"\n== Training for bucket '{bname}', words={len(wlist)} ==")
    best_model, best_K, stats = train_hmm_with_K_sweep(wlist, K_options=K_options, n_iter=n_iter, test_size=0.15, verbose=False)
    key = f"B_{bname}"
    models[key] = {'model': best_model, 'type': 'bucket', 'bucket': bname, 'best_K': best_K, 'n_words': len(wlist), 'val_stats': stats}
    entries = []
    for w in wlist:
        try:
            ll = float(best_model.score(word_to_obs(w)))
        except:
            ll = -1e9
        entries.append((w, ll))
    entries.sort(key=lambda x: x[1], reverse=True)
    logprob_by_key[key] = entries[:50000]

for key, meta in models.items():
    print(f"Model key: {key}  type:{meta['type']}  best_K:{meta['best_K']}  n_words:{meta['n_words']}")
    topwords = logprob_by_key[key][:10]
    print(" Top words:", [w for w,_ in topwords])
    print()

def pattern_matches(word, masked, guessed_wrong_set=None):
    if len(word) != len(masked):
        return False
    for wc, mc in zip(word, masked):
        if mc != '_' and mc != wc:
            return False
    if guessed_wrong_set:
        for ch in guessed_wrong_set:
            if ch in word:
                return False
    return True

def get_key_for_length(L):
    # prefer exact length model if exists, else choose bucket
    key_exact = f"L{L}"
    if key_exact in models:
        return key_exact
    # bucket decision same as earlier
    if L <= 4:
        return "B_short" if "B_short" in models else list(models.keys())[0]
    elif L <= 8:
        return "B_medium" if "B_medium" in models else list(models.keys())[0]
    else:
        return "B_long" if "B_long" in models else list(models.keys())[0]

def get_hmm_prior(masked, guessed_letters=None, top_k=2000):
    """
    masked: string with '_' placeholders, e.g. '_pp_e'
    guessed_letters: can be set() of letters guessed OR dict letter->bool where False indicates guessed & absent
    top_k: consider top_k candidate words by HMM logprob (speed vs accuracy)
    Returns: dict mapping letter -> probability (sums to 1 over letters considered)
    """
    if guessed_letters is None:
        guessed_letters = set()
    guessed_wrong_set = set()
    if isinstance(guessed_letters, dict):
        for ch, present in guessed_letters.items():
            if (not present):
                guessed_wrong_set.add(ch)
    # else guessed_wrong_set remains empty (caller may give explicit wrong set separately)
    L = len(masked)
    key = get_key_for_length(L)
    candidates = []
    entries = logprob_by_key.get(key, [])
    # iterate entries (already sorted by ll desc)
    for w, ll in entries:
        if pattern_matches(w, masked, guessed_wrong_set):
            candidates.append((w, ll))
            if top_k and len(candidates) >= top_k:
                break
    if len(candidates) == 0:
        # fallback: compute simple letter frequency across same-length words not containing wrong guesses
        freqs = Counter()
        total = 0
        for w, _ in entries:
            if len(w) != L:
                continue
            if guessed_wrong_set and any(ch in w for ch in guessed_wrong_set):
                continue
            for i,ch in enumerate(w):
                if masked[i] == '_' and (not (isinstance(guessed_letters, set) and ch in guessed_letters)):
                    freqs[ch]+=1
                    total+=1
        if total == 0:
            # uniform fallback
            return {c:1.0/len(letter2idx) for c in letter2idx}
        return {c: (freqs[c]/total if c in freqs else 0.0) for c in letter2idx}
    # compute weights from loglik
    lls = np.array([ll for _,ll in candidates])
    maxll = np.max(lls)
    weights = np.exp(lls - maxll)
    post = weights / np.sum(weights)
    # marginalize over blanks -> letter mass
    letter_mass = Counter()
    for (w, _), p in zip(candidates, post):
        for i,ch in enumerate(w):
            if masked[i] == '_' and not (isinstance(guessed_letters, set) and ch in guessed_letters):
                letter_mass[ch] += p
    total_mass = sum(letter_mass.values())
    if total_mass == 0:
        return {c:1.0/len(letter2idx) for c in letter2idx}
    probs = {c: (letter_mass[c]/total_mass if c in letter_mass else 0.0) for c in letter2idx}
    return probs

examples = ['_pp_e', 'a__le', '__a__', '_____']
for ex in examples:
    if len(ex) > max_len_cap:
        continue
    probs = get_hmm_prior(ex, guessed_letters=set(), top_k=2000)
    top = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:8]
    print(f"Masked: {ex}  -> Top letters: {top}")

# Print validation stats (we stored val_stats per model)
for key, meta in models.items():
    print(f"Key: {key}  best_K: {meta['best_K']}  n_words: {meta['n_words']}")
    print("Val stats (K -> avg_val_ll):", meta['val_stats'])
    print()

# -----------------------
# Phase 2 Implementation
# -----------------------
import numpy as np
import random
from collections import Counter, defaultdict
from typing import Callable, List, Tuple, Dict, Any

# ---------- Config ----------
LIVES = 6                # number of allowed wrong guesses
MAX_TOPK = 2000          # default top_k used by HMM prior
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# Convenient helpers (requires letter2idx from Phase 1)
ALPHABET = sorted(letter2idx.keys())   # e.g. ['a','b',...]
IDX2LETTER = {i:c for i,c in enumerate(ALPHABET)}
LETTER2IDX = letter2idx
VOCAB_SIZE = len(ALPHABET)

# Build quick lookup: words_by_length (use corpus `words` from Phase 1)
words_by_length = defaultdict(list)
for w in words:
    words_by_length[len(w)].append(w)

# ---------- HangmanEnv ----------
class HangmanEnv:
    """
    Simple Gym-like Hangman environment.
    Observation (returned in 'obs' dict):
      - 'masked_word': string with '_' for blanks
      - 'guessed_mask': np.array shape (VOCAB_SIZE,) binary (1 if guessed)
      - 'hmm_prior': np.array shape (VOCAB_SIZE,) float (from get_hmm_prior)
      - 'lives_left': int
    Action:
      - integer 0..VOCAB_SIZE-1 corresponding to letter in ALPHABET
    Reward semantics (aligned with Phase1 design):
      - correct guess: +0.1
      - wrong guess: -5
      - repeated guess: -2
      - win (bonus on episode end): +1
    """
    def __init__(self, test_words: List[str], lives: int = LIVES, seed: int = SEED):
        assert len(test_words) > 0, "test_words should be non-empty"
        self.test_words = list(test_words)
        self.lives = lives
        self._rng = random.Random(seed)
        self.reset()

    def seed(self, s: int):
        self._rng = random.Random(s)

    def reset(self, sample_idx: int = None):
        """
        If sample_idx is None, pick a random word from test_words.
        Returns initial observation.
        """
        if sample_idx is None:
            self.current_word = self._rng.choice(self.test_words)
        else:
            self.current_word = self.test_words[sample_idx % len(self.test_words)]
        self.word_len = len(self.current_word)
        self.masked = ['_'] * self.word_len
        self.guessed = np.zeros(VOCAB_SIZE, dtype=np.int8)  # 1 if guessed
        self.lives_left = self.lives
        self.done = False
        self.info = {'wrong_guesses':0, 'repeated_guesses':0, 'guesses':[]}
        obs = self._get_obs()
        return obs

    def _get_masked_str(self) -> str:
        return ''.join(self.masked)

    def _get_obs(self) -> Dict[str, Any]:
        masked_str = self._get_masked_str()
        # get HMM prior for the masked string (use Phase1 helper)
        try:
            prior_dict = get_hmm_prior(masked_str, guessed_letters=set(), top_k=MAX_TOPK)
        except Exception as e:
            # fallback to uniform if get_hmm_prior is unavailable
            prior_dict = {c: 1.0/VOCAB_SIZE for c in ALPHABET}
        prior_arr = np.array([prior_dict.get(c, 0.0) for c in ALPHABET], dtype=np.float32)
        obs = {
            'masked_word': masked_str,
            'guessed_mask': self.guessed.copy(),
            'hmm_prior': prior_arr,
            'lives_left': self.lives_left
        }
        return obs

    def step(self, action: int) -> Tuple[Dict[str, Any], float, bool, Dict]:
        """
        action: integer index into ALPHABET
        returns: obs, reward, done, info
        """
        assert 0 <= action < VOCAB_SIZE, "invalid action index"
        if self.done:
            raise RuntimeError("Episode is done. Call reset().")

        letter = ALPHABET[action]
        reward = 0.0
        # repeated guess?
        if self.guessed[action] == 1:
            reward += -2.0
            self.info['repeated_guesses'] += 1
            self.info['guesses'].append((letter, 'repeated'))
            # episode continues; no state change
        else:
            # mark guessed
            self.guessed[action] = 1
            if letter in self.current_word:
                # reveal all occurrences
                for i,ch in enumerate(self.current_word):
                    if ch == letter:
                        self.masked[i] = letter
                reward += 0.1
                self.info['guesses'].append((letter, 'correct'))
            else:
                # wrong guess
                self.lives_left -= 1
                reward += -5.0
                self.info['wrong_guesses'] += 1
                self.info['guesses'].append((letter, 'wrong'))

        # check termination
        if '_' not in self.masked:
            # win
            reward += 1.0  # end bonus
            self.done = True
            self.info['win'] = True
        elif self.lives_left <= 0:
            self.done = True
            self.info['win'] = False
        else:
            self.done = False

        obs = self._get_obs()
        return obs, reward, self.done, dict(self.info)  # return copy of info

    # convenience: run an episode using an agent function that returns action index
    def run_episode(self, agent_fn: Callable[[Dict[str, Any]], int], max_steps: int = 26):
        obs = self.reset()
        total_reward = 0.0
        steps = 0
        while True:
            action = agent_fn(obs)
            obs, r, done, info = self.step(action)
            total_reward += r
            steps += 1
            if done or steps >= max_steps:
                break
        # final info includes final word and guesses
        info['final_word'] = self.current_word
        info['masked_final'] = self._get_masked_str()
        return total_reward, info

# ---------- Baselines ----------
# 1) Frequency baseline: pick highest-frequency unguessed letter globally
global_letter_freq = Counter(''.join(words))
freq_order = [ch for ch,_ in global_letter_freq.most_common() if ch in ALPHABET]
# ensure fallback: letters not in freq_order at end
for ch in ALPHABET:
    if ch not in freq_order:
        freq_order.append(ch)

def freq_baseline(obs: Dict[str, Any]) -> int:
    guessed = obs['guessed_mask']
    for ch in freq_order:
        idx = LETTER2IDX[ch]
        if guessed[idx] == 0:
            return idx
    # fallback (shouldn't happen)
    return 0

# 2) HMM greedy baseline: choose highest-probability letter from get_hmm_prior
def hmm_greedy_baseline(obs: Dict[str, Any], top_k: int = MAX_TOPK) -> int:
    prior = obs['hmm_prior']  # numpy array aligned with ALPHABET
    guessed = obs['guessed_mask']
    # mask guessed letters
    masked_prior = prior.copy()
    masked_prior[guessed == 1] = -1.0
    # pick argmax
    best_idx = int(np.argmax(masked_prior))
    return best_idx

# 3) Pattern-frequency baseline: filter corpus by visible pattern and count letter freq
def pattern_freq_baseline(obs: Dict[str, Any]) -> int:
    masked = obs['masked_word']
    guessed_mask = obs['guessed_mask']
    L = len(masked)
    candidates = words_by_length.get(L, [])
    # quick filter
    filtered = []
    for w in candidates:
        ok = True
        for i,ch in enumerate(masked):
            if ch != '_' and w[i] != ch:
                ok = False; break
        if ok:
            filtered.append(w)
    # count letters in blanks
    freqs = Counter()
    for w in filtered:
        for i,ch in enumerate(w):
            if masked[i] == '_' and guessed_mask[LETTER2IDX[ch]] == 0:
                freqs[ch] += 1
    if len(freqs) == 0:
        return freq_baseline(obs)
    # pick highest unguessed
    picked = freqs.most_common()
    for ch, _ in picked:
        idx = LETTER2IDX[ch]
        if guessed_mask[idx] == 0:
            return idx
    return freq_baseline(obs)

# ---------- Evaluation harness ----------
def evaluate_agent(agent_fn: Callable[[Dict[str, Any]], int],
                   test_words: List[str],
                   n_games: int = 1000,
                   verbose: bool = True) -> Dict[str, Any]:
    """
    Runs agent_fn on the first n_games from test_words (or sampled if shorter).
    Returns dict with metrics: success_rate, total_wrong, total_repeats, avg_wrong, avg_repeats, proxy_score
    Note: proxy_score is a tunable scalar combining wins and penalties; change formula if contest formula differs.
    """
    env = HangmanEnv(test_words, lives=LIVES, seed=SEED)
    total_wins = 0
    total_wrong = 0
    total_repeats = 0
    per_game = []
    # choose n_games samples (deterministic first n or random sample)
    indices = list(range(len(test_words)))
    if n_games < len(indices):
        indices = indices[:n_games]
    else:
        # if fewer words than n_games, wrap-around sampling
        indices = [i % len(test_words) for i in range(n_games)]
    for idx in indices:
        env.seed(SEED + idx)  # different seed per game for deterministic behavior
        env.reset(sample_idx=idx)
        # play until done
        done = False
        while not done:
            obs = env._get_obs()
            action = agent_fn(obs)
            obs, r, done, info = env.step(action)
        info = info  # from final step
        per_game.append(info)
        if info.get('win', False):
            total_wins += 1
        total_wrong += info.get('wrong_guesses', 0)
        total_repeats += info.get('repeated_guesses', 0)

    success_rate = total_wins / len(indices)
    avg_wrong = total_wrong / len(indices)
    avg_repeats = total_repeats / len(indices)

    # Default proxy scoring formula (you can change to official one):
    # score = success_rate * 1000 - 5 * total_wrong - 2 * total_repeats
    proxy_score = success_rate * 1000.0 - 5.0 * total_wrong - 2.0 * total_repeats

    results = {
        'n_games': len(indices),
        'wins': total_wins,
        'success_rate': success_rate,
        'total_wrong': total_wrong,
        'total_repeats': total_repeats,
        'avg_wrong_per_game': avg_wrong,
        'avg_repeats_per_game': avg_repeats,
        'proxy_score': proxy_score,
        'per_game': per_game  # can be large; inspect if needed
    }
    if verbose:
        print(f"Games: {results['n_games']}, Wins: {results['wins']}, Success rate: {results['success_rate']:.4f}")
        print(f"Total wrong: {results['total_wrong']}, Total repeated: {results['total_repeats']}")
        print(f"Avg wrong/game: {results['avg_wrong_per_game']:.3f}, Avg repeats/game: {results['avg_repeats_per_game']:.3f}")
        print(f"Proxy score (custom): {results['proxy_score']:.2f}")
    return results

# ---------- Quick smoke tests (single game) ----------
if __name__ == "__main__":  # running in notebook cell still executes this block
    # play one game with HMM greedy baseline and show steps
    env = HangmanEnv(words, lives=LIVES, seed=SEED)
    obs = env.reset(sample_idx=0)
    print("Target word:", env.current_word)
    done = False
    steps = 0
    while not done and steps < 26:
        action = hmm_greedy_baseline(obs)
        obs, r, done, info = env.step(action)
        print(f"Step {steps+1}: guess {ALPHABET[action]}, reward {r:.2f}, masked -> {obs['masked_word']}")
        steps += 1
    print("Final info:", info)

# Clean each word: keep only characters a-z (lowercase)
def clean_word_keep_alpha(w):
    return ''.join([ch for ch in w.lower() if 'a' <= ch <= 'z'])

cleaned_words = []
for w in words:
    cw = clean_word_keep_alpha(w)
    if cw:  # drop empty results
        cleaned_words.append(cw)

print("Original words count:", len(words))
print("Cleaned (kept non-empty) words count:", len(cleaned_words))

# deduplicate while preserving order
seen = set()
cleaned_unique = []
for w in cleaned_words:
    if w not in seen:
        seen.add(w)
        cleaned_unique.append(w)
print("Unique cleaned words:", len(cleaned_unique))

# show some before/after examples for verification
print("\nSome before -> after examples (first 40):")
for orig in list(dict.fromkeys(words))[:40]:  # unique preserve order
    new = clean_word_keep_alpha(orig)
    if new != orig:
        print(repr(orig), "->", repr(new))

# Replace global words list (in-memory)
words = cleaned_unique.copy()

# rebuild words_by_length
from collections import defaultdict
words_by_length = defaultdict(list)
for w in words:
    words_by_length[len(w)].append(w)

# Update alphabet and mappings if desired (should remain a-z)
ALPHABET = sorted(set(''.join(words)))
LETTER2IDX = {c:i for i,c in enumerate(ALPHABET)}
IDX2LETTER = {i:c for c,i in LETTER2IDX.items()}
VOCAB_SIZE = len(ALPHABET)

print("Updated global variables:")
print(" - total words:", len(words))
print(" - unique alphabet:", ''.join(ALPHABET))
print(" - vocab size:", VOCAB_SIZE)
print(" - sample words_by_length counts (len 1..10):", {L: len(words_by_length[L]) for L in range(1,11)})

# Runtime setup: install torch if not installed
# Adjust the torch install for your CUDA runtime if needed.
import sys
IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    # CPU / GPU auto selection: this will install a CPU-only wheel if colab runtime mismatch.
    !pip install -q torch torchvision
else:
    print("Not running in Colab. Make sure torch is installed in your environment.")

import torch
print("Torch version:", torch.__version__)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Hangman environment wrapper and state vector builder for DQN
import random
import math
import numpy as np
from collections import deque, namedtuple
import time

# safety checks & fallbacks
if 'letter2idx' not in globals():
    raise RuntimeError("letter2idx not found. Make sure your uploaded script defines letter2idx.")
if 'idx2letter' not in globals():
    # build idx2letter if only letter2idx exists
    idx2letter = {v:k for k,v in letter2idx.items()}
if 'V' not in globals():
    V = len(letter2idx)
if 'max_len_cap' not in globals():
    max_len_cap = max(len(w) for w in words)

# Environment
class HangmanEnv:
    def __init__(self, word, max_wrong=6, max_len=max_len_cap, letter2idx=letter2idx):
        self.word = word
        self.max_wrong = max_wrong
        self.max_len = max_len
        self.letter2idx = letter2idx
        self.V = len(letter2idx)
        self.reset(word)

    def reset(self, word=None):
        if word is not None:
            self.word = word
        self.mask = ['_'] * len(self.word)
        self.guessed = set()
        self.wrong = 0
        self.done = False
        self.repeated_guesses = 0
        return self.get_state()

    def step(self, action_letter):
        if self.done:
            raise RuntimeError("step() called on finished env")
        info = {}
        reward = 0.0
        # repeated guess?
        if action_letter in self.guessed:
            self.repeated_guesses += 1
            reward += -0.35
            info['repeated'] = True
            return self.get_state(), reward, self.done, info
        self.guessed.add(action_letter)
        if action_letter in self.word:
            for i,ch in enumerate(self.word):
                if ch == action_letter:
                    self.mask[i] = ch
            reward += 0.12
        else:
            self.wrong += 1
            reward += -0.15
        reward += -0.005
        if '_' not in self.mask:
            self.done = True
            reward += 1.0
            info['result'] = 'win'
        elif self.wrong >= self.max_wrong:
            self.done = True
            reward += -1.0
            info['result'] = 'loss'
        return self.get_state(), reward, self.done, info

    def get_state(self):
        masked = ''.join(self.mask)
        lives_left = self.max_wrong - self.wrong
        guessed_letters = set(self.guessed)
        # prepare guessed dict for HMM prior if HMM function expects presence info
        guessed_dict = {}
        for ch in self.letter2idx.keys():
            if ch in guessed_letters:
                guessed_dict[ch] = (ch in self.word)
        # attempt to call get_hmm_prior from your script (fallback to uniform)
        try:
            hmm_prior = get_hmm_prior(masked, guessed_letters=guessed_dict, top_k=2000)
        except Exception as e:
            # fallback uniform distribution
            hmm_prior = {c: 1.0/self.V for c in self.letter2idx.keys()}
        return {'masked': masked, 'guessed_set': guessed_letters, 'lives_left': lives_left, 'hmm_prior': hmm_prior}

# Input builder
def build_input_vector(state):
    masked = state['masked']
    guessed_set = state['guessed_set']
    hmm_prior = state['hmm_prior']
    lives = state['lives_left']
    max_len = max_len_cap
    Vloc = len(letter2idx)
    masked_oh = np.zeros((max_len, Vloc), dtype=np.float32)
    for i in range(max_len):
        if i < len(masked):
            ch = masked[i]
            if ch != '_':
                idx = letter2idx[ch]
                masked_oh[i, idx] = 1.0
    masked_flat = masked_oh.flatten()
    guessed_bin = np.zeros(Vloc, dtype=np.float32)
    for ch in guessed_set:
        if ch in letter2idx:
            guessed_bin[letter2idx[ch]] = 1.0
    hmm_vec = np.array([hmm_prior.get(idx2letter[i], 0.0) for i in range(Vloc)], dtype=np.float32)
    lives_norm = np.array([lives / 6.0], dtype=np.float32)
    inp = np.concatenate([masked_flat, guessed_bin, hmm_vec, lives_norm])
    return inp

# DQN, replay buffer, agent (PyTorch)
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DQNNet(nn.Module):
    def __init__(self, input_dim, output_dim, hidden=256):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fc3 = nn.Linear(hidden, output_dim)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))
class ReplayBuffer:
    def __init__(self, capacity=100000):
        self.buf = deque(maxlen=capacity)
    def push(self, *args):
        self.buf.append(Transition(*args))
    def sample(self, batch_size):
        batch = random.sample(self.buf, batch_size)
        return Transition(*zip(*batch))
    def __len__(self):
        return len(self.buf)

class DQNAgent:
    def __init__(self, input_dim, action_dim, lr=1e-3, gamma=0.99):
        self.action_dim = action_dim
        self.online = DQNNet(input_dim, action_dim).to(device)
        self.target = DQNNet(input_dim, action_dim).to(device)
        self.target.load_state_dict(self.online.state_dict())
        self.opt = optim.Adam(self.online.parameters(), lr=lr)
        self.gamma = gamma
        self.replay = ReplayBuffer()
        self.steps = 0

    def select_action(self, state_vec, available_mask, eps):
        if random.random() < eps:
            allowed_idxs = [i for i,b in enumerate(available_mask) if b]
            if not allowed_idxs:
                return random.randrange(self.action_dim)
            return random.choice(allowed_idxs)
        self.online.eval()
        with torch.no_grad():
            x = torch.from_numpy(state_vec).float().unsqueeze(0).to(device)
            q = self.online(x).cpu().numpy().flatten()
            q_masked = np.full_like(q, -1e9)
            for i,b in enumerate(available_mask):
                if b:
                    q_masked[i] = q[i]
            act = int(np.argmax(q_masked))
        self.online.train()
        return act

    def push_transition(self, *args):
        self.replay.push(*args)

    def update(self, batch_size=64):
        if len(self.replay) < batch_size:
            return 0.0
        trans = self.replay.sample(batch_size)
        state_b = torch.from_numpy(np.vstack(trans.state)).float().to(device)
        next_state_b = torch.from_numpy(np.vstack(trans.next_state)).float().to(device)
        action_b = torch.tensor(trans.action, dtype=torch.long, device=device).unsqueeze(1)
        reward_b = torch.tensor(trans.reward, dtype=torch.float32, device=device).unsqueeze(1)
        done_b = torch.tensor(trans.done, dtype=torch.float32, device=device).unsqueeze(1)

        q_vals = self.online(state_b).gather(1, action_b)
        with torch.no_grad():
            next_q = self.target(next_state_b).max(1)[0].unsqueeze(1)
            target_q = reward_b + (1.0 - done_b) * self.gamma * next_q
        loss = F.mse_loss(q_vals, target_q)
        self.opt.zero_grad()
        loss.backward()
        self.opt.step()
        self.steps += 1
        return loss.item()

    def sync_target(self):
        self.target.load_state_dict(self.online.state_dict())

# Training and evaluation utilities
import matplotlib.pyplot as plt
from google.colab import files as gfiles

def evaluate_agent(agent, test_words, max_games=2000):
    games = min(len(test_words), max_games)
    successes = 0
    total_wrong = 0
    total_repeated = 0
    for i, w in enumerate(test_words[:games]):
        env = HangmanEnv(w, max_wrong=6)
        state = env.reset()
        state_vec = build_input_vector(state)
        done = False
        steps = 0
        while not done:
            available_mask = np.array([idx2letter[i] not in state['guessed_set'] for i in range(V)])
            act_idx = agent.select_action(state_vec, available_mask, eps=0.0)
            act_letter = idx2letter[act_idx]
            next_state, r, done, info = env.step(act_letter)
            state = next_state
            state_vec = build_input_vector(state)
            steps += 1
            if steps > max_len_cap + 20:
                break
        if 'result' in info and info['result'] == 'win':
            successes += 1
        total_wrong += env.wrong
        total_repeated += env.repeated_guesses
    return {'games': games, 'successes': successes, 'total_wrong': total_wrong, 'total_repeated': total_repeated}

def train_dqn(agent, episodes=5000, eval_every=500, batch_size=128, save_path='dqn_hangman.pt'):
    # prepare dataset split
    all_words = [w for w in words if all(ch in letter2idx for ch in w) and len(w) <= max_len_cap]
    random.shuffle(all_words)
    test_size = min(2000, int(0.15 * len(all_words)))
    test_set = all_words[:test_size]
    train_pool = all_words[test_size:]
    print(f"Train pool size: {len(train_pool)}, Test size: {len(test_set)}")

    input_dim = (max_len_cap * V) + V + V + 1
    print("Input dim:", input_dim, "Action dim:", V)

    eps_start = 1.0
    eps_final = 0.05
    eps_decay = episodes * 0.6

    losses = []
    eval_history = []
    best_score = -1e9

    for ep in range(1, episodes+1):
        word = random.choice(train_pool)
        env = HangmanEnv(word, max_wrong=6)
        state = env.reset()
        state_vec = build_input_vector(state)
        done = False
        total_reward = 0.0
        steps = 0
        while not done:
            eps = eps_final + (eps_start - eps_final) * math.exp(-1.0 * agent.steps / eps_decay)
            available_mask = np.array([idx2letter[i] not in state['guessed_set'] for i in range(V)])
            act_idx = agent.select_action(state_vec, available_mask, eps)
            act_letter = idx2letter[act_idx]
            next_state, r, done, info = env.step(act_letter)
            next_state_vec = build_input_vector(next_state)
            agent.push_transition(state_vec, act_idx, next_state_vec, r, float(done))
            loss = agent.update(batch_size=batch_size)
            if loss:
                losses.append(loss)
            state_vec = next_state_vec
            state = next_state
            total_reward += r
            steps += 1
            if steps > max_len_cap + 10:
                break
        if ep % 200 == 0:
            agent.sync_target()
        if ep % eval_every == 0 or ep == episodes:
            eval_metrics = evaluate_agent(agent, test_set, max_games=len(test_set))
            success_rate = eval_metrics['successes'] / eval_metrics['games']
            total_wrong = eval_metrics['total_wrong']
            total_repeated = eval_metrics['total_repeated']
            final_score = (success_rate * 2000.0) - (total_wrong * 5.0) - (total_repeated * 2.0)
            eval_history.append((ep, success_rate, total_wrong, total_repeated, final_score))
            print(f"[EP {ep}] SuccessRate: {success_rate:.3f}, wrong: {total_wrong}, repeated: {total_repeated}, FinalScore: {final_score:.1f}")
            if final_score > best_score:
                best_score = final_score
                torch.save(agent.online.state_dict(), save_path)
                print("Saved best model:", save_path)
    # final save
    torch.save(agent.online.state_dict(), save_path)
    return losses, eval_history, best_score

# Create agent and run a short training experiment.
# Increase episodes to 20k+ for better performance; this is a quick demo.
episodes = 3000        # quick run; set to 20000+ for production
eval_every = 500
batch_size = 128
input_dim = (max_len_cap * V) + V + V + 1

agent = DQNAgent(input_dim, V, lr=1e-3, gamma=0.98)
start = time.time()
losses, eval_history, best_score = train_dqn(agent, episodes=episodes, eval_every=eval_every, batch_size=batch_size, save_path='dqn_hangman.pt')
print("Training finished in {:.1f}s. Best score: {}".format(time.time()-start, best_score))

# Simple plots for loss (rolling) and evaluation history
import matplotlib.pyplot as plt
plt.figure(figsize=(10,4))
if len(losses)>0:
    plt.plot(losses, alpha=0.7)
    plt.title("DQN training losses (per update)")
    plt.xlabel("updates")
    plt.ylabel("MSE loss")
else:
    plt.text(0.2, 0.5, "No losses recorded â€” not enough replay samples yet", fontsize=12)
plt.show()

# eval history
if eval_history:
    eps, succ, wrong, rep, score = zip(*eval_history)
    plt.figure(figsize=(10,4))
    plt.plot(eps, succ, marker='o', label='success rate')
    plt.plot(eps, score, marker='x', label='final score')
    plt.xlabel("episode")
    plt.legend()
    plt.title("Evaluation history")
    plt.show()
else:
    print("No evaluation checkpoints recorded.")

# Final evaluation using the best saved model
from pathlib import Path
model_path = Path('dqn_hangman.pt')
if model_path.exists():
    agent.online.load_state_dict(torch.load(str(model_path), map_location=device))
    agent.sync_target()
    print("Loaded saved model:", model_path)
else:
    print("Model file not found; using current agent weights.")

# prepare test set
all_words = [w for w in words if all(ch in letter2idx for ch in w) and len(w) <= max_len_cap]
random.shuffle(all_words)
test_set = all_words[:min(2000, len(all_words))]
metrics = evaluate_agent(agent, test_set, max_games=len(test_set))
success_rate = metrics['successes'] / metrics['games']
final_score = (success_rate * 2000.0) - (metrics['total_wrong'] * 5.0) - (metrics['total_repeated'] * 2.0)
print("Final eval on held-out set:", metrics)
print(f"Success rate: {success_rate:.4f}, Final Score: {final_score:.2f}")

# provide download link for the model
from google.colab import files
files.download(str(model_path))